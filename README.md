# ğŸ”¥ PromptForge

**PromptForge** is a portfolioâ€‘grade **AI Red Teaming & Prompt Injection Testing Framework** designed to evaluate the security posture of Large Language Models (LLMs) against adversarial prompt-based attacks.

> âš”ï¸ *Weaponizing prompts to test AI defenses.*

---

## ğŸ“Œ What is PromptForge?

PromptForge is an **offensive AI security lab** that simulates real-world **prompt injection, role confusion, and policy bypass attacks** against LLMs under different security configurations.

It is built for:
- AI Red Teaming
- LLM Security Research
- Prompt Injection Practice
- Model Defense Benchmarking
- Security Portfolios

---

## ğŸš€ Key Features

### Core Capabilities
- âœ… Goal-based AI exploitation challenges  
- âœ… Multiple security levels (Easy vs Hard)  
- âœ… Automatic attack success detection  
- âœ… Attempt-based scoring system  
- âœ… Attack logging & replay  
- âœ… Modular & extensible architecture  

### Advanced Capabilities
- ğŸ”¥ Prompt injection attacks  
- ğŸ”¥ Role confusion attacks  
- ğŸ”¥ Obfuscated prompt attacks  
- ğŸ”¥ Defense benchmarking  
- ğŸ”¥ Model-agnostic design (future-ready)  

---

## ğŸ§  Challenges Included

| Challenge ID | Name | Goal |
|------------|------|------|
| 1 | Phrase Injection | Force model to output a restricted phrase |
| 2 | Secret Extraction | Extract sensitive file contents |
| 3 | Unauthorized Transaction | Bypass financial restrictions |

Each challenge simulates **real AI security failure modes**.

---

## ğŸ” Security Levels

| Level | Description |
|-----|-------------|
| Easy | Weak defenses, trusts user input |
| Hard | Injection-aware, refusal logic enabled |

This allows **defense strength comparison**.

---

## ğŸ“ Project Structure
```
PromptForge/
â”‚
â”œâ”€â”€ core/ # Core engine & logic
â”‚ â”œâ”€â”€ engine.py
â”‚ â”œâ”€â”€ evaluator.py
â”‚ â”œâ”€â”€ models.py
â”‚ â”œâ”€â”€ prompts.py
â”‚ â””â”€â”€ scoring.py
â”‚
â”œâ”€â”€ attacks/ # Attack payload generators
â”‚ â”œâ”€â”€ injection.py
â”‚ â”œâ”€â”€ role_confusion.py
â”‚ â””â”€â”€ obfuscation.py
â”‚
â”œâ”€â”€ challenges/ # Challenge definitions
â”‚ â”œâ”€â”€ challenge_1.py
â”‚ â”œâ”€â”€ challenge_2.py
â”‚ â””â”€â”€ challenge_3.py
â”‚
â”œâ”€â”€ logs/ # Attack logs
â”‚ â””â”€â”€ attacks.json
â”‚
â”œâ”€â”€ main.py # Entry point
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository
```
git clone https://github.com/<your-username>/PromptForge.git
cd PromptForge
2ï¸âƒ£ Create Virtual Environment (Recommended) :
python3 -m venv venv
source venv/bin/activate
3ï¸âƒ£ Install Dependencies :
pip install -r requirements.txt
ğŸ”‘ API Configuration :
export GEMINI_API_KEY="your_api_key_here"
â–¶ï¸ Usage :
python main.py
```

ğŸ§ª Attack Logging
All attacks are automatically logged to:
```
logs/attacks.json
```
Each log includes:
- Timestamp
- Challenge ID
- Prompt payload
- Model response
- Success status

ğŸ›¡ï¸ Ethical Use Notice :
PromptForge is intended strictly for defensive security testing, research, and education.

ğŸ§‘â€ğŸ’» Author :
Faseeh Ul Hassan
AI Red Teaming | LLM Security | Prompt Injection Research

ğŸ“Œ GitHub: @faseeh-ul-hassan
ğŸ“Œ Focus: Agentic AI Security, Prompt Injection, Model Exploitation

ğŸ“œ License :
This project is licensed under the MIT License.
See LICENSE file for details.

â­ Why This Project Matters :

PromptForge demonstrates:

- Offensive security mindset
- Understanding of LLM failure modes
- Real AI Red Teaming workflows
- Clean engineering & modular design

